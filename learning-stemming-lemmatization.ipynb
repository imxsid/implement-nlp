{"cells":[{"source":"<a href=\"https://www.kaggle.com/sid9300/learning-stemming-lemmatization?scriptVersionId=84370057\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"6c35ca23","metadata":{"papermill":{"duration":0.024813,"end_time":"2022-01-04T07:50:29.936666","exception":false,"start_time":"2022-01-04T07:50:29.911853","status":"completed"},"tags":[]},"source":["* [Prerequisite](#pre)\n","    * [Importing Libraries](#importing)\n","    * [Reading Sample Texts](#reading)\n","    * [Extracting Tokens](#extracting)\n","* [Tokenisation](#tokenisation)\n","    * [Word Tokenisation](#word)\n","    * [Sentence Tokenisation](#sentence)\n","    * [Tweet Tokenisation](#tweet)\n","    * [Custom Tokenisation (using Regex)](#regex)\n","* [Stemmer](#stemmer) \n","    * [Porter Stemmer](#porter)\n","    * [Snowball Stemmer](#snowball)\n","* [Lemmatizer](#lemmatizer)\n","    * [Wordnet Lemmatizer](#wordnet)\n","* [Conclusion](#conclusion)"]},{"cell_type":"markdown","id":"9de4c10a","metadata":{"papermill":{"duration":0.019476,"end_time":"2022-01-04T07:50:29.975856","exception":false,"start_time":"2022-01-04T07:50:29.95638","status":"completed"},"tags":[]},"source":["## <font color='#4a8bad'>Prerequisite</font>\n","***\n","<a id=\"pre\"></a>"]},{"cell_type":"markdown","id":"6980620b","metadata":{"papermill":{"duration":0.019547,"end_time":"2022-01-04T07:50:30.014915","exception":false,"start_time":"2022-01-04T07:50:29.995368","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Importing Libraries</font>\n","<a id=\"importing\"></a>"]},{"cell_type":"code","execution_count":1,"id":"1658848c","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:30.065191Z","iopub.status.busy":"2022-01-04T07:50:30.06447Z","iopub.status.idle":"2022-01-04T07:50:31.719615Z","shell.execute_reply":"2022-01-04T07:50:31.720216Z","shell.execute_reply.started":"2022-01-04T07:49:21.646466Z"},"papermill":{"duration":1.685099,"end_time":"2022-01-04T07:50:31.72056","exception":false,"start_time":"2022-01-04T07:50:30.035461","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import TweetTokenizer\n","from nltk.tokenize import regexp_tokenize\n","\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem.snowball import SnowballStemmer\n","\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"markdown","id":"fd1160f8","metadata":{"papermill":{"duration":0.019063,"end_time":"2022-01-04T07:50:31.759355","exception":false,"start_time":"2022-01-04T07:50:31.740292","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Reading Sample Texts</font>\n","<a id=\"reading\"></a>"]},{"cell_type":"code","execution_count":2,"id":"fedb8d2e","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:31.804378Z","iopub.status.busy":"2022-01-04T07:50:31.803502Z","iopub.status.idle":"2022-01-04T07:50:31.80706Z","shell.execute_reply":"2022-01-04T07:50:31.807777Z","shell.execute_reply.started":"2022-01-04T07:49:21.653933Z"},"papermill":{"duration":0.029558,"end_time":"2022-01-04T07:50:31.80803","exception":false,"start_time":"2022-01-04T07:50:31.778472","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Text :  250\n","------------------\n","Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\n"]}],"source":["text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n","\n","print(\"Text : \", len(text))\n","print(\"------------------\")\n","print(text)"]},{"cell_type":"code","execution_count":3,"id":"73497f5d","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:31.855598Z","iopub.status.busy":"2022-01-04T07:50:31.854493Z","iopub.status.idle":"2022-01-04T07:50:31.861374Z","shell.execute_reply":"2022-01-04T07:50:31.861902Z","shell.execute_reply.started":"2022-01-04T07:49:21.666431Z"},"papermill":{"duration":0.031954,"end_time":"2022-01-04T07:50:31.862121","exception":false,"start_time":"2022-01-04T07:50:31.830167","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Text :  113\n","------------------\n","Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\n"]}],"source":["document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n","print(\"Text : \", len(document))\n","print(\"------------------\")\n","print(text)"]},{"cell_type":"code","execution_count":4,"id":"1be9a26e","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:31.910206Z","iopub.status.busy":"2022-01-04T07:50:31.907057Z","iopub.status.idle":"2022-01-04T07:50:31.912989Z","shell.execute_reply":"2022-01-04T07:50:31.913502Z","shell.execute_reply.started":"2022-01-04T07:49:21.67596Z"},"papermill":{"duration":0.030968,"end_time":"2022-01-04T07:50:31.913674","exception":false,"start_time":"2022-01-04T07:50:31.882706","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Text :  117\n","------------------\n","i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\n"]}],"source":["message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n","print(\"Text : \", len(message))\n","print(\"------------------\")\n","print(message)"]},{"cell_type":"markdown","id":"a493d1e5","metadata":{"papermill":{"duration":0.021135,"end_time":"2022-01-04T07:50:31.955762","exception":false,"start_time":"2022-01-04T07:50:31.934627","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Extracting Tokens</font>\n","<a id=\"extracting\"></a>"]},{"cell_type":"code","execution_count":5,"id":"cc29b362","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.004583Z","iopub.status.busy":"2022-01-04T07:50:32.001841Z","iopub.status.idle":"2022-01-04T07:50:32.025845Z","shell.execute_reply":"2022-01-04T07:50:32.025233Z","shell.execute_reply.started":"2022-01-04T07:49:21.687976Z"},"papermill":{"duration":0.048793,"end_time":"2022-01-04T07:50:32.026031","exception":false,"start_time":"2022-01-04T07:50:31.977238","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens :  47\n","------------------\n","['very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pitted', 'its', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"]}],"source":["tokens = word_tokenize(text.lower())\n","print(\"Tokens : \", len(tokens))\n","print(\"------------------\")\n","print(tokens)"]},{"cell_type":"markdown","id":"0e8502f3","metadata":{"papermill":{"duration":0.02074,"end_time":"2022-01-04T07:50:32.067568","exception":false,"start_time":"2022-01-04T07:50:32.046828","status":"completed"},"tags":[]},"source":["## <font color='#4a8bad'>Tokenisation</font>\n","***\n","<a id=\"tokenisation\"></a>"]},{"cell_type":"markdown","id":"0f9efaec","metadata":{"papermill":{"duration":0.02072,"end_time":"2022-01-04T07:50:32.111177","exception":false,"start_time":"2022-01-04T07:50:32.090457","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Word Tokenisation</font>\n","<a id=\"word\"></a>\n","\n","NLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token."]},{"cell_type":"code","execution_count":6,"id":"7f1baa75","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.164214Z","iopub.status.busy":"2022-01-04T07:50:32.163446Z","iopub.status.idle":"2022-01-04T07:50:32.167225Z","shell.execute_reply":"2022-01-04T07:50:32.166327Z","shell.execute_reply.started":"2022-01-04T07:49:21.69807Z"},"papermill":{"duration":0.033752,"end_time":"2022-01-04T07:50:32.167416","exception":false,"start_time":"2022-01-04T07:50:32.133664","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Words :  25\n","------------------\n","['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"]}],"source":["words = word_tokenize(document)\n","print(\"Words : \", len(words))\n","print(\"------------------\")\n","print(words)"]},{"cell_type":"markdown","id":"0b87480b","metadata":{"papermill":{"duration":0.022826,"end_time":"2022-01-04T07:50:32.213858","exception":false,"start_time":"2022-01-04T07:50:32.191032","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Sentence Tokenisation</font>\n","<a id=\"sentence\"></a>\n","\n","Tokenising based on sentence requires you to split on the period ('.'). Let's use nltk sentence tokeniser."]},{"cell_type":"code","execution_count":7,"id":"e642b0ca","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.262825Z","iopub.status.busy":"2022-01-04T07:50:32.262145Z","iopub.status.idle":"2022-01-04T07:50:32.266522Z","shell.execute_reply":"2022-01-04T07:50:32.265892Z","shell.execute_reply.started":"2022-01-04T07:49:21.711372Z"},"papermill":{"duration":0.031064,"end_time":"2022-01-04T07:50:32.266674","exception":false,"start_time":"2022-01-04T07:50:32.23561","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentences :  2\n","------------------\n","[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"]}],"source":["sentences = sent_tokenize(document)\n","print(\"Sentences : \", len(sentences))\n","print(\"------------------\")\n","print(sentences)"]},{"cell_type":"markdown","id":"837d21cb","metadata":{"papermill":{"duration":0.022474,"end_time":"2022-01-04T07:50:32.311745","exception":false,"start_time":"2022-01-04T07:50:32.289271","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Tweet Tokenisation</font>\n","<a id=\"tweet\"></a>\n","\n","A problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags. Emojis are common these days and people use them all the time.\n","Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is.\n","\n","Let's use the tweet tokeniser of nltk to tokenise this message."]},{"cell_type":"code","execution_count":8,"id":"a66b3a00","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.363161Z","iopub.status.busy":"2022-01-04T07:50:32.362303Z","iopub.status.idle":"2022-01-04T07:50:32.36563Z","shell.execute_reply":"2022-01-04T07:50:32.366185Z","shell.execute_reply.started":"2022-01-04T07:49:21.72223Z"},"papermill":{"duration":0.032275,"end_time":"2022-01-04T07:50:32.366367","exception":false,"start_time":"2022-01-04T07:50:32.334092","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tweets :  23\n","------------------\n","['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':)', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<3', '.', '#bingewatching', '#nothingtodo', 'üòé']\n"]}],"source":["tweets = TweetTokenizer().tokenize(message)\n","print(\"Tweets : \", len(tweets))\n","print(\"------------------\")\n","print(tweets)"]},{"cell_type":"markdown","id":"f138926f","metadata":{"papermill":{"duration":0.022214,"end_time":"2022-01-04T07:50:32.410654","exception":false,"start_time":"2022-01-04T07:50:32.38844","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Custom Tokenisation (using Regex)</font>\n","<a id=\"regex\"></a>\n","\n","Now, there is a tokeniser that takes a regular expression and tokenises and returns result based on the pattern of regular expression.\n","\n","Let's look at how you can use regular expression tokeniser."]},{"cell_type":"code","execution_count":9,"id":"23e75002","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.463811Z","iopub.status.busy":"2022-01-04T07:50:32.463037Z","iopub.status.idle":"2022-01-04T07:50:32.466766Z","shell.execute_reply":"2022-01-04T07:50:32.466107Z","shell.execute_reply.started":"2022-01-04T07:49:21.733139Z"},"papermill":{"duration":0.033494,"end_time":"2022-01-04T07:50:32.466925","exception":false,"start_time":"2022-01-04T07:50:32.433431","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Words :  2\n","------------------\n","['#bingewatching', '#nothingtodo']\n"]}],"source":["pattern = \"#[\\w]+\"\n","\n","words_regex = regexp_tokenize(message, pattern)\n","print(\"Words : \", len(words_regex))\n","print(\"------------------\")\n","print(words_regex)"]},{"cell_type":"markdown","id":"5a5c2451","metadata":{"papermill":{"duration":0.02238,"end_time":"2022-01-04T07:50:32.512036","exception":false,"start_time":"2022-01-04T07:50:32.489656","status":"completed"},"tags":[]},"source":["## <font color='#4a8bad'>Stemmer</font>\n","***\n","<a id=\"stemmer\"></a>"]},{"cell_type":"markdown","id":"98118c3f","metadata":{"papermill":{"duration":0.02341,"end_time":"2022-01-04T07:50:32.558581","exception":false,"start_time":"2022-01-04T07:50:32.535171","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Porter Stemmer</font>\n","<a id=\"porter\"></a>"]},{"cell_type":"code","execution_count":10,"id":"464c10bc","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.613826Z","iopub.status.busy":"2022-01-04T07:50:32.613043Z","iopub.status.idle":"2022-01-04T07:50:32.615923Z","shell.execute_reply":"2022-01-04T07:50:32.616453Z","shell.execute_reply.started":"2022-01-04T07:49:21.742528Z"},"papermill":{"duration":0.034424,"end_time":"2022-01-04T07:50:32.616624","exception":false,"start_time":"2022-01-04T07:50:32.5822","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens :  47\n","------------------\n","['veri', 'orderli', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'hi', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"]}],"source":["stemmer = PorterStemmer()\n","porter_stemmed = [stemmer.stem(token) for token in tokens]\n","\n","print(\"Tokens : \", len(porter_stemmed))\n","print(\"------------------\")\n","print(porter_stemmed)"]},{"cell_type":"markdown","id":"2996f2ef","metadata":{"papermill":{"duration":0.022642,"end_time":"2022-01-04T07:50:32.663109","exception":false,"start_time":"2022-01-04T07:50:32.640467","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Snowball Stemmer</font>\n","<a id=\"snowball\"></a>"]},{"cell_type":"code","execution_count":11,"id":"06609885","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.715774Z","iopub.status.busy":"2022-01-04T07:50:32.715067Z","iopub.status.idle":"2022-01-04T07:50:32.717528Z","shell.execute_reply":"2022-01-04T07:50:32.718063Z","shell.execute_reply.started":"2022-01-04T07:49:21.756644Z"},"papermill":{"duration":0.031954,"end_time":"2022-01-04T07:50:32.718235","exception":false,"start_time":"2022-01-04T07:50:32.686281","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens :  47\n","------------------\n","['veri', 'order', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'his', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"]}],"source":["stemmer = SnowballStemmer(\"english\")\n","snowball_stemmed = [stemmer.stem(token) for token in tokens]\n","\n","print(\"Tokens : \", len(snowball_stemmed))\n","print(\"------------------\")\n","print(snowball_stemmed)"]},{"cell_type":"markdown","id":"e101e210","metadata":{"papermill":{"duration":0.022187,"end_time":"2022-01-04T07:50:32.763057","exception":false,"start_time":"2022-01-04T07:50:32.74087","status":"completed"},"tags":[]},"source":["## <font color='#4a8bad'>Lemmatizer</font>\n","***\n","<a id=\"lemmatizer\"></a>"]},{"cell_type":"markdown","id":"d7491ba7","metadata":{"papermill":{"duration":0.02264,"end_time":"2022-01-04T07:50:32.808465","exception":false,"start_time":"2022-01-04T07:50:32.785825","status":"completed"},"tags":[]},"source":["#### <font color='#4a8bad'>Wordnet Lemmatizer</font>\n","<a id=\"wordnet\"></a>"]},{"cell_type":"code","execution_count":12,"id":"0dcb16c7","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:32.859803Z","iopub.status.busy":"2022-01-04T07:50:32.856881Z","iopub.status.idle":"2022-01-04T07:50:35.031051Z","shell.execute_reply":"2022-01-04T07:50:35.030468Z","shell.execute_reply.started":"2022-01-04T07:49:21.766249Z"},"papermill":{"duration":2.199682,"end_time":"2022-01-04T07:50:35.03123","exception":false,"start_time":"2022-01-04T07:50:32.831548","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens :  47\n","------------------\n","['very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'a', 'though', 'it', 'pitted', 'it', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"]}],"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","lemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n","\n","print(\"Tokens : \", len(lemmatized))\n","print(\"------------------\")\n","print(lemmatized)"]},{"cell_type":"markdown","id":"2b11040e","metadata":{"papermill":{"duration":0.025787,"end_time":"2022-01-04T07:50:35.080418","exception":false,"start_time":"2022-01-04T07:50:35.054631","status":"completed"},"tags":[]},"source":["## <font color='#4a8bad'>Conclusion</font>\n","***\n","<a id=\"conclusion\"></a>"]},{"cell_type":"code","execution_count":13,"id":"4ec60328","metadata":{"execution":{"iopub.execute_input":"2022-01-04T07:50:35.140416Z","iopub.status.busy":"2022-01-04T07:50:35.139676Z","iopub.status.idle":"2022-01-04T07:50:35.175651Z","shell.execute_reply":"2022-01-04T07:50:35.176197Z","shell.execute_reply.started":"2022-01-04T07:49:21.779272Z"},"papermill":{"duration":0.067877,"end_time":"2022-01-04T07:50:35.176383","exception":false,"start_time":"2022-01-04T07:50:35.108506","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Differences :  16\n","------------------\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokens</th>\n","      <th>porter_stemmed</th>\n","      <th>snowball_stemmed</th>\n","      <th>lemmatized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>very</td>\n","      <td>veri</td>\n","      <td>veri</td>\n","      <td>very</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>orderly</td>\n","      <td>orderli</td>\n","      <td>order</td>\n","      <td>orderly</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>methodical</td>\n","      <td>method</td>\n","      <td>method</td>\n","      <td>methodical</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>looked</td>\n","      <td>look</td>\n","      <td>look</td>\n","      <td>looked</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>ticking</td>\n","      <td>tick</td>\n","      <td>tick</td>\n","      <td>ticking</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>sonorous</td>\n","      <td>sonor</td>\n","      <td>sonor</td>\n","      <td>sonorous</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>his</td>\n","      <td>hi</td>\n","      <td>his</td>\n","      <td>his</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>flapped</td>\n","      <td>flap</td>\n","      <td>flap</td>\n","      <td>flapped</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>newly</td>\n","      <td>newli</td>\n","      <td>newli</td>\n","      <td>newly</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>as</td>\n","      <td>as</td>\n","      <td>as</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>pitted</td>\n","      <td>pit</td>\n","      <td>pit</td>\n","      <td>pitted</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>its</td>\n","      <td>it</td>\n","      <td>it</td>\n","      <td>it</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>gravity</td>\n","      <td>graviti</td>\n","      <td>graviti</td>\n","      <td>gravity</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>longevity</td>\n","      <td>longev</td>\n","      <td>longev</td>\n","      <td>longevity</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>levity</td>\n","      <td>leviti</td>\n","      <td>leviti</td>\n","      <td>levity</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>evanescence</td>\n","      <td>evanesc</td>\n","      <td>evanesc</td>\n","      <td>evanescence</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         tokens porter_stemmed snowball_stemmed   lemmatized\n","0          very           veri             veri         very\n","1       orderly        orderli            order      orderly\n","3    methodical         method           method   methodical\n","5        looked           look             look       looked\n","18      ticking           tick             tick      ticking\n","20     sonorous          sonor            sonor     sonorous\n","23          his             hi              his          his\n","24      flapped           flap             flap      flapped\n","25        newly          newli            newli        newly\n","29           as             as               as            a\n","32       pitted            pit              pit       pitted\n","33          its             it               it           it\n","34      gravity        graviti          graviti      gravity\n","36    longevity         longev           longev    longevity\n","39       levity         leviti           leviti       levity\n","41  evanescence        evanesc          evanesc  evanescence"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.DataFrame({\"tokens\": tokens, \"porter_stemmed\" : porter_stemmed, \"snowball_stemmed\" : snowball_stemmed, \"lemmatized\" : lemmatized})\n","df_new = df[(df.tokens != df.porter_stemmed) | (df.tokens != df.snowball_stemmed) | (df.tokens != lemmatized)]\n","\n","print(\"Differences : \", len(df_new))\n","print(\"------------------\")\n","df_new"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":16.043401,"end_time":"2022-01-04T07:50:36.212993","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-01-04T07:50:20.169592","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}